---
title: 'Text_Summary'
date: 2025-02-03
permalink: /posts/2025/02/Text_Summary/
tags:
  - deeplearning
---

<section id="nice" data-tool="mdnice编辑器" data-website="https://www.mdnice.com"><p data-tool="mdnice编辑器">​</p>
<p data-tool="mdnice编辑器">文本摘要的常见问题和解决方法概述，以及使用Hugging Face Transformers库构建基于新浪微博数据集的文本摘要示例。</p>

<h2 data-tool="mdnice编辑器"><span class="prefix"></span><span class="content">1 前言简介</span><span class="suffix"></span></h2>
<p data-tool="mdnice编辑器">文本摘要旨在将文本或文本集合转换为包含关键信息的简短文本。主流方法有两种类型，抽取式和生成式。常见问题：抽取式摘要的内容选择错误、语句连贯性差、灵活性差。生成式摘要受未登录词、词语重复等问题影响。
文本摘要的分类有很多，比如单文档多文档摘要、多语言摘要、论文生成（摘要、介绍、重点陈述等每个章节的生成）、医学报告生成、情感类摘要（观点、感受、评价等的摘要）、对话摘要等。主流解决方法主要是基于深度学习、强化学习、迁移学习等方法，有大量的相关论文可以解读和研究。
抽取式的代表方法有TextRank、BertSum[1]，从原文中抽取出字词组合成新的摘要。TextRank仿照PageRank，句子作为节点，构造无向有权边,权值为句子相似度。
生成式摘要方法是有PGN[2]、GPT、BART[3]、BRIO[4]、GSum[5]、SimCLS[6]、CIT+SE[7]等。
对于生成式摘要，不得不简单再提一下PGN模型：
Pointer Generator Network结构[2]</p>
<p data-tool="mdnice编辑器">生成式存在的一个大问题是OOV未登录词，Google的PGN使用 copy mechanism和Coverage mechanism，能对于未遇见过的词直接复制用于生成部分，比如上面的“2-0”，同时避免了重复生成。</p>
<h2 data-tool="mdnice编辑器"><span class="prefix"></span><span class="content">2 数据集</span><span class="suffix"></span></h2>
<p data-tool="mdnice编辑器">文本摘要的数据集有很多，这里使用的是Lcstsm[10]大规模中文短文本摘要语料库，取自于新浪微博，训练集共有240万条，为了快速得到结果和理解过程，可以自己设置采用数据的大小。比如训练集设置10万条。</p>
<h2 data-tool="mdnice编辑器"><span class="prefix"></span><span class="content">3 模型选型</span><span class="suffix"></span></h2>
<p data-tool="mdnice编辑器">预训练模型选的是"csebuetnlp/mT5_multilingual_XLSum"，是ACL-IJCNLP 2021的论文XL-Sum[8]中开源的模型，他们也是基于多语言T5模型 (mT5)在自己准备的44中语言的摘要数据集上做了fine-tune得到的模型。mT5[9]是按照与T5类似的方法进行训练的大规模多语言预训练模型。
如何加载模型？使用Hugging Face Transformers库可以两行代码即可加载。
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM</p>
<p data-tool="mdnice编辑器">model_checkpoint = "csebuetnlp/mT5_multilingual_XLSum"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)</p>
<h2 data-tool="mdnice编辑器"><span class="prefix"></span><span class="content">4 数据预处理</span><span class="suffix"></span></h2>
<p data-tool="mdnice编辑器">加载模型和tokenizer后对自定义文本分词：</p>
<figure data-tool="mdnice编辑器"><img src="https://files.mdnice.com/user/38578/098a57ba-45e9-4c88-8746-354f279d2ef2.png" alt=""></figure>
<p data-tool="mdnice编辑器">mT5 模型采用的是基于 Unigram 切分的 SentencePiece 分词器，Unigram 对于处理多语言语料库特别有用。SentencePiece 可以在不知道重音、标点符号以及没有空格分隔字符（例如中文）的情况下对文本进行分词。</p>
<p data-tool="mdnice编辑器">摘要任务的输入和标签都是文本，所以我们要做这几件事：
1、使用 as_target_tokenizer() 函数并行地对输入和标签进行分词，并标签序列中填充的 pad 字符设置为 -100 ，这样可以在计算交叉熵损失时忽略掉；</p>
<p data-tool="mdnice编辑器">2、对标签进行移位操作，来准备 decoder_input_ids，有封装好的prepare_decoder_input_ids_from_labels函数。</p>
<p data-tool="mdnice编辑器">3、将每一个 batch 中的数据都处理为模型可接受的输入格式：包含 'attention_mask'、'input_ids'、'labels' 和 'decoder_input_ids' 键的字典。</p>
<p data-tool="mdnice编辑器">在编程的时候，可以打印出一个 batch 的数据看看：</p>
<figure data-tool="mdnice编辑器"><img src="https://files.mdnice.com/user/38578/14a8c598-605e-4bd0-af1d-89cd576bc2e2.png" alt=""></figure>
<h2 data-tool="mdnice编辑器"><span class="prefix"></span><span class="content">5 模型训练和评测</span><span class="suffix"></span></h2>
<p data-tool="mdnice编辑器">因为Transformers包已经帮我们封装好了模型、损失函数等内容，我们只需调用并定义好训练循环即可：</p>
<pre class="custom" data-tool="mdnice编辑器"><code class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span>&nbsp;<span class="hljs-title">train_loop</span><span class="hljs-params">(dataloader,&nbsp;model,&nbsp;optimizer,&nbsp;lr_scheduler,&nbsp;epoch,&nbsp;total_loss)</span>:</span><br>&nbsp;&nbsp;&nbsp;&nbsp;progress_bar&nbsp;=&nbsp;tqdm(range(len(dataloader)))<br>&nbsp;&nbsp;&nbsp;&nbsp;progress_bar.set_description(<span class="hljs-string">f'loss:&nbsp;<span class="hljs-subst">{<span class="hljs-number">0</span>:&gt;<span class="hljs-number">7</span>f}</span>'</span>)<br>&nbsp;&nbsp;&nbsp;&nbsp;finish_batch_num&nbsp;=&nbsp;(epoch<span class="hljs-number">-1</span>)&nbsp;*&nbsp;len(dataloader)<br>&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;model.train()<br>&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span>&nbsp;batch,&nbsp;batch_data&nbsp;<span class="hljs-keyword">in</span>&nbsp;enumerate(dataloader,&nbsp;start=<span class="hljs-number">1</span>):<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;batch_data&nbsp;=&nbsp;batch_data.to(device)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;outputs&nbsp;=&nbsp;model(**batch_data)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss&nbsp;=&nbsp;outputs.loss<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;optimizer.zero_grad()<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;loss.backward()<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;optimizer.step()<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lr_scheduler.step()<br><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;total_loss&nbsp;+=&nbsp;loss.item()<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;progress_bar.set_description(<span class="hljs-string">f'loss:&nbsp;<span class="hljs-subst">{total_loss/(finish_batch_num&nbsp;+&nbsp;batch):&gt;<span class="hljs-number">7</span>f}</span>'</span>)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;progress_bar.update(<span class="hljs-number">1</span>)<br>&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span>&nbsp;total_loss<br><br></code></pre>
<p data-tool="mdnice编辑器">训练的过程中，不断的优化模型权重参数，那么如何评估模型的性能、如何确保模型往更好的方向优化呢？用什么评估指标呢？</p>
<p data-tool="mdnice编辑器">我们定义一个测试循环负责评估模型的性能，指标就使用ROUGE(Recall-Oriented Understudy for Gisting Evaluation)，它可以度量两个词语序列之间的词语重合率。</p>
<p data-tool="mdnice编辑器">ROUGE 值的召回率，表示<em>被参考摘要 reference summary</em>有多大程度上被<em>生成摘要 generated summary</em>覆盖，精确率则表示<em>生成摘要</em>中有多少词语与<em>被参考摘要</em>相关。</p>
<p data-tool="mdnice编辑器">那么，如果我们只比较词语，召回率是：
Recall = 重叠词数/被参考摘要总词数
Precision=重叠词数/生成摘要的总词数</p>
<p data-tool="mdnice编辑器">已经有rouge的Python包[11]，使用pip安装即可</p>
<pre class="custom" data-tool="mdnice编辑器"><code class="hljs">pip&nbsp;install&nbsp;rouge<br></code></pre>
<figure data-tool="mdnice编辑器"><img src="https://files.mdnice.com/user/38578/ae18bfa6-0f4e-47c7-9d54-ca1ba55f2a84.png" alt=""></figure>
<p data-tool="mdnice编辑器">rouge库官方示例，"f" stands for f1_score, "p" stands for precision, "r" stands for recall.</p>
<p data-tool="mdnice编辑器">ROUGE-1 度量 uni-grams 的重合情况，ROUGE-2 度量 bi-grams 的重合情况，而 ROUGE-L 则通过在生成摘要和参考摘要中寻找最长公共子串来度量最长的单词匹配序列。</p>
<p data-tool="mdnice编辑器">另外要注意的是，rouge 库默认使用空格进行分词，中文需要按字切分，也可以使用分词器分词后再计算。</p>
<figure data-tool="mdnice编辑器"><img src="https://files.mdnice.com/user/38578/8465f78b-eb98-4074-9198-9c43ece13ec0.png" alt=""></figure>
<p data-tool="mdnice编辑器">Transformers对解码过程也进行了封装，我们只需要调用 generate() 函数就可以自动地逐个生成预测 token。</p>
<p data-tool="mdnice编辑器">例如我们使用马来亚大学的介绍生成一个摘要：“马来亚大学是马来西亚历史最悠久的高等教育学府。“</p>
<figure data-tool="mdnice编辑器"><img src="https://files.mdnice.com/user/38578/64fee87b-a2a2-4b2a-ae79-dfed6ab1715f.png" alt=""></figure>
<p data-tool="mdnice编辑器">所以，在测试过程中，我们通过generate() 函数获取预测结果，然后将预测结果和正确标签都处理为 rouge 库接受的格式，最后计算各项的ROUGE值即可：</p>
<pre class="custom" data-tool="mdnice编辑器"><code class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span>&nbsp;<span class="hljs-title">test_loop</span><span class="hljs-params">(dataloader,&nbsp;model,&nbsp;tokenizer)</span>:</span><br>&nbsp;&nbsp;&nbsp;&nbsp;preds,&nbsp;labels&nbsp;=&nbsp;[],&nbsp;[]<br>&nbsp;&nbsp;&nbsp;&nbsp;rouge&nbsp;=&nbsp;Rouge()<br>&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;model.eval()<br>&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">with</span>&nbsp;torch.no_grad():<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">for</span>&nbsp;batch_data&nbsp;<span class="hljs-keyword">in</span>&nbsp;tqdm(dataloader):<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;batch_data&nbsp;=&nbsp;batch_data.to(device)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-comment">#&nbsp;获取预测结果</span><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;generated_tokens&nbsp;=&nbsp;model.generate(batch_data[<span class="hljs-string">"input_ids"</span>],<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;attention_mask=batch_data[<span class="hljs-string">"attention_mask"</span>],<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_length=max_target_length,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;num_beams=beam_search_size,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;no_repeat_ngram_size=no_repeat_ngram_size,<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;).cpu().numpy()<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">if</span>&nbsp;isinstance(generated_tokens,&nbsp;tuple):<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;generated_tokens&nbsp;=&nbsp;generated_tokens[<span class="hljs-number">0</span>]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;decoded_preds&nbsp;=&nbsp;tokenizer.batch_decode(generated_tokens,&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;skip_special_tokens=<span class="hljs-literal">True</span>,&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;clean_up_tokenization_spaces=<span class="hljs-literal">False</span>)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;label_tokens&nbsp;=&nbsp;batch_data[<span class="hljs-string">"labels"</span>].cpu().numpy()<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-comment">#&nbsp;将标签序列中的&nbsp;-100&nbsp;替换为&nbsp;pad&nbsp;token&nbsp;ID&nbsp;以便于分词器解码</span><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;label_tokens&nbsp;=&nbsp;np.where(label_tokens&nbsp;!=&nbsp;<span class="hljs-number">-100</span>,&nbsp;label_tokens,&nbsp;tokenizer.pad_token_id)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;decoded_labels&nbsp;=&nbsp;tokenizer.batch_decode(label_tokens,&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;skip_special_tokens=<span class="hljs-literal">True</span>,&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;clean_up_tokenization_spaces=<span class="hljs-literal">False</span>)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-comment">#&nbsp;处理为&nbsp;rouge&nbsp;库接受的文本列表格式</span><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;preds&nbsp;+=&nbsp;[<span class="hljs-string">'&nbsp;'</span>.join(pred.strip())&nbsp;<span class="hljs-keyword">for</span>&nbsp;pred&nbsp;<span class="hljs-keyword">in</span>&nbsp;decoded_preds]<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;labels&nbsp;+=&nbsp;[<span class="hljs-string">'&nbsp;'</span>.join(label.strip())&nbsp;<span class="hljs-keyword">for</span>&nbsp;label&nbsp;<span class="hljs-keyword">in</span>&nbsp;decoded_labels]<br>&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-comment">#&nbsp;rouge&nbsp;库计算各项&nbsp;ROUGE&nbsp;值</span><br>&nbsp;&nbsp;&nbsp;&nbsp;scores&nbsp;=&nbsp;rouge.get_scores(hyps=preds,&nbsp;refs=labels,&nbsp;avg=<span class="hljs-literal">True</span>)<br>&nbsp;&nbsp;&nbsp;&nbsp;result&nbsp;=&nbsp;{key:&nbsp;value[<span class="hljs-string">'f'</span>]&nbsp;*&nbsp;<span class="hljs-number">100</span>&nbsp;<span class="hljs-keyword">for</span>&nbsp;key,&nbsp;value&nbsp;<span class="hljs-keyword">in</span>&nbsp;scores.items()}<br>&nbsp;&nbsp;&nbsp;&nbsp;result[<span class="hljs-string">'avg'</span>]&nbsp;=&nbsp;np.mean(list(result.values()))<br>&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span>&nbsp;result<br><br><br></code></pre>
<h2 data-tool="mdnice编辑器"><span class="prefix"></span><span class="content">6 模型保存</span><span class="suffix"></span></h2>
<p data-tool="mdnice编辑器">优化器我们选使用AdamW，并且通过 get_scheduler()函数定义学习率调度器。</p>
<p data-tool="mdnice编辑器">在每一个epoch中，调用上面定义的train_loop和test_loop，模型在验证集上的rouge分数用来调整超参数和选出最好的模型，最后使用最好的模型跑测一下测试集来评估最终的性能。</p>
<pre class="custom" data-tool="mdnice编辑器"><code class="hljs"><span class="hljs-string">"""&nbsp;Train&nbsp;the&nbsp;model&nbsp;"""</span><br>total_steps&nbsp;=&nbsp;len(train_dataloader)&nbsp;*&nbsp;num_train_epochs<br><span class="hljs-comment">#&nbsp;Prepare&nbsp;optimizer&nbsp;and&nbsp;schedule&nbsp;(linear&nbsp;warmup&nbsp;and&nbsp;decay)</span><br>no_decay&nbsp;=&nbsp;[<span class="hljs-string">"bias"</span>,&nbsp;<span class="hljs-string">"LayerNorm.weight"</span>]<br>optimizer_grouped_parameters&nbsp;=&nbsp;[<br>&nbsp;&nbsp;&nbsp;&nbsp;{<span class="hljs-string">"params"</span>:&nbsp;[p&nbsp;<span class="hljs-keyword">for</span>&nbsp;n,&nbsp;p&nbsp;<span class="hljs-keyword">in</span>&nbsp;model.named_parameters()&nbsp;<span class="hljs-keyword">if</span>&nbsp;<span class="hljs-keyword">not</span>&nbsp;any(nd&nbsp;<span class="hljs-keyword">in</span>&nbsp;n&nbsp;<span class="hljs-keyword">for</span>&nbsp;nd&nbsp;<span class="hljs-keyword">in</span>&nbsp;no_decay)],&nbsp;<span class="hljs-string">"weight_decay"</span>:&nbsp;weight_decay},<br>&nbsp;&nbsp;&nbsp;&nbsp;{<span class="hljs-string">"params"</span>:&nbsp;[p&nbsp;<span class="hljs-keyword">for</span>&nbsp;n,&nbsp;p&nbsp;<span class="hljs-keyword">in</span>&nbsp;model.named_parameters()&nbsp;<span class="hljs-keyword">if</span>&nbsp;any(nd&nbsp;<span class="hljs-keyword">in</span>&nbsp;n&nbsp;<span class="hljs-keyword">for</span>&nbsp;nd&nbsp;<span class="hljs-keyword">in</span>&nbsp;no_decay)],&nbsp;<span class="hljs-string">"weight_decay"</span>:&nbsp;<span class="hljs-number">0.0</span>}<br>]<br>warmup_steps&nbsp;=&nbsp;int(total_steps&nbsp;*&nbsp;warmup_proportion)<br>optimizer&nbsp;=&nbsp;AdamW(<br>&nbsp;&nbsp;&nbsp;&nbsp;optimizer_grouped_parameters,&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;lr=learning_rate,&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;betas=(adam_beta1,&nbsp;adam_beta2),&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;eps=adam_epsilon<br>)<br>lr_scheduler&nbsp;=&nbsp;get_scheduler(<br>&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">'linear'</span>,<br>&nbsp;&nbsp;&nbsp;&nbsp;optimizer,&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;num_warmup_steps=warmup_steps,<br>&nbsp;&nbsp;&nbsp;&nbsp;num_training_steps=total_steps<br>)<br><br><span class="hljs-comment">#&nbsp;Train!</span><br>logger.info(<span class="hljs-string">"*****&nbsp;Running&nbsp;training&nbsp;*****"</span>)<br>logger.info(<span class="hljs-string">f"Num&nbsp;examples&nbsp;-&nbsp;<span class="hljs-subst">{len(train_data)}</span>"</span>)<br>logger.info(<span class="hljs-string">f"Num&nbsp;Epochs&nbsp;-&nbsp;<span class="hljs-subst">{num_train_epochs}</span>"</span>)<br>logger.info(<span class="hljs-string">f"Total&nbsp;optimization&nbsp;steps&nbsp;-&nbsp;<span class="hljs-subst">{total_steps}</span>"</span>)<br><br>total_loss&nbsp;=&nbsp;<span class="hljs-number">0.</span><br>best_avg_rouge&nbsp;=&nbsp;<span class="hljs-number">0.</span><br><span class="hljs-keyword">for</span>&nbsp;epoch&nbsp;<span class="hljs-keyword">in</span>&nbsp;range(num_train_epochs):<br>&nbsp;&nbsp;&nbsp;&nbsp;print(<span class="hljs-string">f"Epoch&nbsp;<span class="hljs-subst">{epoch+<span class="hljs-number">1</span>}</span>/<span class="hljs-subst">{num_train_epochs}</span>\n"</span>&nbsp;+&nbsp;<span class="hljs-number">30</span>&nbsp;*&nbsp;<span class="hljs-string">"-"</span>)<br>&nbsp;&nbsp;&nbsp;&nbsp;total_loss&nbsp;=&nbsp;train_loop(train_dataloader,&nbsp;model,&nbsp;optimizer,&nbsp;lr_scheduler,&nbsp;epoch,&nbsp;total_loss)<br>&nbsp;&nbsp;&nbsp;&nbsp;dev_rouges&nbsp;=&nbsp;test_loop(dev_dataloader,&nbsp;model,&nbsp;tokenizer)<br>&nbsp;&nbsp;&nbsp;&nbsp;logger.info(<span class="hljs-string">f"Dev&nbsp;Rouge1:&nbsp;<span class="hljs-subst">{dev_rouges[<span class="hljs-string">'rouge-1'</span>]:&gt;<span class="hljs-number">0.2</span>f}</span>&nbsp;Rouge2:&nbsp;<span class="hljs-subst">{dev_rouges[<span class="hljs-string">'rouge-2'</span>]:&gt;<span class="hljs-number">0.2</span>f}</span>&nbsp;RougeL:&nbsp;<span class="hljs-subst">{dev_rouges[<span class="hljs-string">'rouge-l'</span>]:&gt;<span class="hljs-number">0.2</span>f}</span>"</span>)<br>&nbsp;&nbsp;&nbsp;&nbsp;rouge_avg&nbsp;=&nbsp;dev_rouges[<span class="hljs-string">'avg'</span>]<br>&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">if</span>&nbsp;rouge_avg&nbsp;&gt;&nbsp;best_avg_rouge:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;best_avg_rouge&nbsp;=&nbsp;rouge_avg<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;logger.info(<span class="hljs-string">f'saving&nbsp;new&nbsp;weights&nbsp;to&nbsp;<span class="hljs-subst">{output_dir}</span>...\n'</span>)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;save_weight&nbsp;=&nbsp;<span class="hljs-string">f'epoch_<span class="hljs-subst">{epoch+<span class="hljs-number">1</span>}</span>_rouge_<span class="hljs-subst">{rouge_avg:<span class="hljs-number">0.4</span>f}</span>_weights.bin'</span><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;torch.save(model.state_dict(),&nbsp;os.path.join(output_dir,&nbsp;save_weight))<br>logger.info(<span class="hljs-string">"Done!"</span>)<br><br><br></code></pre>
<h2 data-tool="mdnice编辑器"><span class="prefix"></span><span class="content">7 总结</span><span class="suffix"></span></h2>
<p data-tool="mdnice编辑器">文本摘要和文本生成是自然语言处理中非常重要和常见的任务，本文使用生成式方法做文本摘要，文本生成还可以应用于其他场景下，比如给定一个句子，生成多个与其相似、语义相同的句子，这里也Mark一下，后面再写一篇相关文章。</p>
<p data-tool="mdnice编辑器">文本摘要还存在很多问题，有待研究者们进一步探索，比如：长程依赖、新颖性、暴露偏差、评估和损失不匹配、缺乏概括性、虚假事实、不连贯等，此外，还有摘要任务特定的问题，比如：如何确定关键信息，而不仅仅是简单地句子压缩。每个问题的解决或方法优化都能发表论文，相关的论文有太多了。后续可以抽关键的解决方案聊聊。</p>
<p data-tool="mdnice编辑器">代码近期整理后上传Github，链接见文末留言处。</p>
<p data-tool="mdnice编辑器">欢迎文末留言，随意交流~</p>
<p data-tool="mdnice编辑器">end</p>

<section class="footnotes" data-tool="mdnice编辑器">
<span id="fn1" class="footnote-item"><span class="footnote-num">[1] </span><p>Get To The Point: Summarization with Pointer-Generator Networks: <em>http://arxiv.org/abs/1704.04368</em></p>
</span>
<span id="fn2" class="footnote-item"><span class="footnote-num">[2] </span><p>BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.: <em>https://aclanthology.org/2020.acl-main.703.pdf</em></p>
</span>
<span id="fn3" class="footnote-item"><span class="footnote-num">[3] </span><p>BRIO: Bringing Order to Abstractive Summarization: <em>https://arxiv.org/abs/2203.16804v1</em></p>
</span>
<span id="fn4" class="footnote-item"><span class="footnote-num">[4] </span><p>GSum: A General Framework for Guided Neural Abstractive Summarization: <em>https://arxiv.org/abs/2010.08014</em></p>
</span>
<span id="fn5" class="footnote-item"><span class="footnote-num">[5] </span><p>SimCLS: A Simple Framework for Contrastive Learning of Abstractive Summarization: <em>https://arxiv.org/abs/2106.01890v1</em></p>
</span>
<span id="fn6" class="footnote-item"><span class="footnote-num">[6] </span><p>Abstractive Summarization with Combination of Pre-trained Sequence-to-Sequence and Saliency Models: <em>https://arxiv.org/abs/2003.13028</em></p>
</span>
<span id="fn7" class="footnote-item"><span class="footnote-num">[7] </span><p>XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages: <em>https://github.com/csebuetnlp/xl-sum</em></p>
</span>
<span id="fn8" class="footnote-item"><span class="footnote-num">[8] </span><p>mT5: A massively multilingual pre-trained text-to-text transformer: <em>https://github.com/google-research/multilingual-t5</em></p>
</span>
<span id="fn9" class="footnote-item"><span class="footnote-num">[9] </span><p>Lcsts: A large scale chinese short text summarization dataset: <em>https://arxiv.org/pdf/1506.05865.pdf</em></p>
</span>
<span id="fn10" class="footnote-item"><span class="footnote-num">[10] </span><p>rouge: <em>https://github.com/pltrdy/rouge</em></p>
</span>
<span id="fn11" class="footnote-item"><span class="footnote-num">[11] </span><p>summarization: <em>https://xiaosheng.run/</em></p>
</span>
<span id="fn12" class="footnote-item"><span class="footnote-num">[12] </span><p>Deep reinforcement and transfer learning for abstractive text summarization: <em>https://www.sciencedirect.com/science/article/abs/pii/S0885230821000796</em></p>
</span>
<span id="fn13" class="footnote-item"><span class="footnote-num">[13] </span><p>Summarization Papers: <em>https://github.com/xcfcode/Summarization-Papers</em></p>
</span>
</section>
</section>