---
title: 'Pturning'
date: 2025-02-03
permalink: /posts/2025/02/pturning/
tags:
  - finetune
---

本文主要内容
==============
Bert时代，我们常做<strong>预训练模型微调（Fine-tuning）</strong>，即根据不同下游任务，引入各种辅助任务loss和垂直领域数据，将其添加到预训练模型中，以便让模型更加适配下游任务的方式。每个下游任务都存下整个预训练模型的副本，并且推理必须在单独的批次中执行。那么能不能<strong>将所有自然语言处理的任务转换为语言模型任务</strong>？就是所有任务都可以被统一建模，任务描述与任务输入视为语言模型的历史上下文，而输出则为语言模型需要预测的未来信息。

<p data-tool="mdnice编辑器">因此，<span class="footnote-word">Prompt</span><sup class="footnote-ref">[1]</sup>新范式被提出，无需要fine-tune，让预训练模型直接适应下游任务。Prompt方式更加依赖先验，而 fine-tuning 更加依赖后验。</p>

P-tuning
==============
本文所提到的P-tuning有两个版本。

<p data-tool="mdnice编辑器">论文<span class="footnote-word">GPT Understands, Too</span><sup class="footnote-ref">[2]</sup>中的Prompt tuning，在本文行文过程中称为<strong>P-tuning v1</strong>，对应GitHub 代码：</p>
<p data-tool="mdnice编辑器">https://github.com/THUDM/P-tuning</p>
<p data-tool="mdnice编辑器"><span class="footnote-word">P-Tuning v2</span><sup class="footnote-ref">[3]</sup>在论文《P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks》中提出，GitHub代码：</p>
<p data-tool="mdnice编辑器">https://github.com/THUDM/P-tuning-v2</p>
<h2 data-tool="mdnice编辑器"><span class="prefix"></span><span class="content">prefix-tuning</span><span class="suffix"></span></h2>
<p data-tool="mdnice编辑器">如果分析 P-tuning，那不得不提到<span class="footnote-word">prefix-tuning技术</span><sup class="footnote-ref">[4]</sup>，相对于fine-tuning，在调节模型的过程中只优化一小段可学习的continuous task-specific vector（prefix）而不是整个模型的参数。</p>
<p data-tool="mdnice编辑器">对于不同的任务和模型结构需要不同的<span class="footnote-word">prefix</span><sup class="footnote-ref">[5]</sup>：</p>
<ul data-tool="mdnice编辑器">
<li><section><p>在autoregressive LM 前添加prefix获得：
<img src="https://files.mdnice.com/user/38578/22daa5ee-e2ef-408d-b9fe-cba995585484.png" alt=""></p>
</section></li><li><section><p>在encoder和decoder之前添加prefixs获得：
<img src="https://files.mdnice.com/user/38578/f1d4a935-adbf-43d3-9268-219086c28162.png" alt=""></p>
</section></li></ul>
<h2 data-tool="mdnice编辑器"><span class="prefix"></span><span class="content">P-tuning v1</span><span class="suffix"></span></h2>
<figure data-tool="mdnice编辑器"><img src="https://files.mdnice.com/user/38578/ddde13e8-979c-43e7-9d7f-c4ad4f11f349.png" alt=""></figure>
<p data-tool="mdnice编辑器">主要结构是利用了一个prompt encoder（BiLSTM+MLP），将一些pseudo prompt先encode（离散token）再与input embedding进行拼接，同时利用LSTM进行 Reparamerization 加速训练，并引入少量自然语言提示的锚字符（Anchor，例如Britain）进一步提升效果。然后结合（capital，Britain）生成得到结果，再优化生成的encoder部分。</p>
<p data-tool="mdnice编辑器">P-tuning v1有两个显著缺点：<strong>任务不通用和规模不通用</strong></p>
<p data-tool="mdnice编辑器">在一些复杂的自然语言理解NLU任务上效果很差，比如序列标注等；预训练模型的参数量不能小，仅在10B规模表现良好，而在稍小规模的模型（330M和2B）上表现不佳。</p>
<h2 data-tool="mdnice编辑器"><span class="prefix"></span><span class="content">P-tuning v2</span><span class="suffix"></span></h2>
<p data-tool="mdnice编辑器">V2版本主要是基于P-tuning和prefix-tuning技术，引入Deep Prompt Encoding和Multi-task Learning等策略进行优化的。</p>
<ul data-tool="mdnice编辑器">
<li><section>仅精调0.1%参数量，在330M到10B不同参数规模LM模型上，均取得和Fine-tuning相比肩的性能：</section></li></ul>
<figure data-tool="mdnice编辑器"><img src="https://files.mdnice.com/user/38578/e4f53be4-8c23-4fbc-8117-cbd11cf85019.png" alt=""></figure>
<ul data-tool="mdnice编辑器">
<li><section>将Prompt tuning技术首次拓展至<strong>序列标注</strong>等复杂的NLU任务上，而P-tuning(v1)在此任务上无法运作：</section></li></ul>
<figure data-tool="mdnice编辑器"><img src="https://files.mdnice.com/user/38578/18975749-6bc8-456c-b4a4-39b064ca0eab.png" alt=""></figure>
<h2 data-tool="mdnice编辑器"><span class="prefix"></span><span class="content">v1和v2框架对比：</span><span class="suffix"></span></h2>
<figure data-tool="mdnice编辑器"><img src="https://files.mdnice.com/user/38578/1054004f-43b8-4a1f-b464-b42c4531fc5c.png" alt=""></figure>
<p data-tool="mdnice编辑器">可以看到右侧的p-tuning v2中，将continuous prompt加在序列前端，并且每一层都加入可训练的prompts。在左图v1模型中，只将prompt插入input embedding中，会导致可训练的参数被句子的长度所限制。</p>
<p data-tool="mdnice编辑器"><strong>v2变化</strong>：</p>
<p data-tool="mdnice编辑器">移除了Reparameterization，舍弃了词汇Mapping的Verbalizer的使用，重新利用CLS和字符标签，来增强通用性，这样可以适配到序列标注任务。此外，作者还引入了两项技术：</p>
<ul data-tool="mdnice编辑器">
<li><section><strong>Deep Prompt Encoding</strong>：</section></li></ul>
<p data-tool="mdnice编辑器">采用 Prefix-tuning 的做法，在输入前面的每层加入可微调的参数。使用无重参数化编码器对pseudo token，不再使用重参数化进行表征（如用于 prefix-tunning 的 MLP 和用于 P-tuning 的 LSTM），且不再替换pre-trained word embedding，取而代之的是直接对pseudo token对应的深层模型的参数进行微调。</p>
<ul data-tool="mdnice编辑器">
<li><section><strong>Multi-task learning</strong>：</section></li></ul>
<p data-tool="mdnice编辑器">基于多任务数据集的Prompt进行预训练，然后再适配到下游任务。对于pseudo token的continous prompt，随机初始化比较难以优化，因此采用multi-task方法同时训练多个数据集，<strong>共享continuous prompts</strong>去进行多任务预训练，可以让prompt有比较好的初始化。</p>
<h1 data-tool="mdnice编辑器"><span class="prefix"></span><span class="content">大模型 p-tuning</span><span class="suffix"></span></h1>
<p data-tool="mdnice编辑器">ptuning v2论文已经证明在不同规模大小模型和不同NLP任务上的有效性，结合最近大模型涌现后的微调热，清华相关实验室对 <span class="footnote-word">ChatGLM-6B 模型</span><sup class="footnote-ref">[6]</sup>做了基于 <span class="footnote-word">P-Tuning v2的微调</span><sup class="footnote-ref">[7]</sup>。</p>
<p data-tool="mdnice编辑器">需要微调的参数量减少到原来的 0.1%，结合模型量化和Gradient Checkpoint 等方法，最低只需要 7GB 显存即可运行了。</p>
<p data-tool="mdnice编辑器">这里使用了两层MLP对Prefix做 Encode：</p>
<pre class="custom" data-tool="mdnice编辑器"><code class="hljs"><span class="hljs-class"><span class="hljs-keyword">class</span>&nbsp;<span class="hljs-title">PrefixEncoder</span><span class="hljs-params">(torch.nn.Module)</span>:</span><br>&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-string">"""<br>&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;torch.nn&nbsp;model&nbsp;to&nbsp;encode&nbsp;the&nbsp;prefix<br>&nbsp;&nbsp;&nbsp;&nbsp;Input&nbsp;shape:&nbsp;(batch-size,&nbsp;prefix-length)<br>&nbsp;&nbsp;&nbsp;&nbsp;Output&nbsp;shape:&nbsp;(batch-size,&nbsp;prefix-length,&nbsp;2*layers*hidden)<br>&nbsp;&nbsp;&nbsp;&nbsp;"""</span><br>&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span>&nbsp;<span class="hljs-title">__init__</span><span class="hljs-params">(self,&nbsp;config)</span>:</span><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;super().__init__()<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.prefix_projection&nbsp;=&nbsp;config.prefix_projection<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">if</span>&nbsp;self.prefix_projection:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-comment">#&nbsp;这里！！</span><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.embedding&nbsp;=&nbsp;torch.nn.Embedding(config.pre_seq_len,&nbsp;config.hidden_size)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.trans&nbsp;=&nbsp;torch.nn.Sequential(<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;torch.nn.Linear(config.hidden_size,&nbsp;config.hidden_size),<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;torch.nn.Tanh(),<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;torch.nn.Linear(config.hidden_size,&nbsp;config.num_layers&nbsp;*&nbsp;config.hidden_size&nbsp;*&nbsp;<span class="hljs-number">2</span>)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">else</span>:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.embedding&nbsp;=&nbsp;torch.nn.Embedding(config.pre_seq_len,&nbsp;config.num_layers&nbsp;*&nbsp;config.hidden_size&nbsp;*&nbsp;<span class="hljs-number">2</span>)<br><br>&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-function"><span class="hljs-keyword">def</span>&nbsp;<span class="hljs-title">forward</span><span class="hljs-params">(self,&nbsp;prefix:&nbsp;torch.Tensor)</span>:</span><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">if</span>&nbsp;self.prefix_projection:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prefix_tokens&nbsp;=&nbsp;self.embedding(prefix)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;past_key_values&nbsp;=&nbsp;self.trans(prefix_tokens)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">else</span>:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;past_key_values&nbsp;=&nbsp;self.embedding(prefix)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="hljs-keyword">return</span>&nbsp;past_key_values<br></code></pre>
<p data-tool="mdnice编辑器">并在 ChatGLMModel class 中的 prompt 处调用：</p>
<p data-tool="mdnice编辑器"><img src="https://files.mdnice.com/user/38578/4ebc450e-3ab6-4da7-bf3c-27b3df5d0829.png" alt="">
并且传入到每一层中：</p>
<figure data-tool="mdnice编辑器"><img src="https://files.mdnice.com/user/38578/8f6fbf66-d87d-4a2c-b67b-d1924a9f2c6d.png" alt=""></figure>
<p data-tool="mdnice编辑器">这时候，我们对照论文中的v2模型结构就更好理解了：
<img src="https://files.mdnice.com/user/38578/64bf2b1b-ad41-4bb6-918e-5b33db326d95.jpg" alt=""></p>
<p data-tool="mdnice编辑器">使用两层MLP对prompt做encode，添加到模型每一层，这部分的参数是可训练的，右侧蓝色部分是预训练模型的权重不做更新。</p>
<section class="footnotes-sep" data-tool="mdnice编辑器"></section>
<section class="footnotes" data-tool="mdnice编辑器">
<span id="fn1" class="footnote-item"><span class="footnote-num">[1] </span><p>Prompt综述: <em>https://dl.acm.org/doi/pdf/10.1145/3560815</em></p>
</span>
<span id="fn2" class="footnote-item"><span class="footnote-num">[2] </span><p>P-Tuning v1 论文: <em>https://arxiv.org/pdf/2103.10385.pdf</em></p>
</span>
<span id="fn3" class="footnote-item"><span class="footnote-num">[3] </span><p>P-Tuning v2论文: <em>https://arxiv.org/abs/2110.07602</em></p>
</span>
<span id="fn4" class="footnote-item"><span class="footnote-num">[4] </span><p>Prefix-Tuning: Optimizing Continuous Prompts for Generation: <em>https://arxiv.org/abs/2101.00190</em></p>
</span>
<span id="fn5" class="footnote-item"><span class="footnote-num">[5] </span><p>PrefixTuning Code: <em>https://github.com/XiangLi1999/PrefixTuning</em></p>
</span>
<span id="fn6" class="footnote-item"><span class="footnote-num">[6] </span><p>ChatGLM-6B: <em>https://github.com/THUDM/ChatGLM-6B</em></p>
</span>
<span id="fn7" class="footnote-item"><span class="footnote-num">[7] </span><p>ChatGLM-6B PTuning: <em>https://github.com/THUDM/ChatGLM-6B/tree/main/ptuning</em></p>
</span>
</section>
</section>